# 인공지능 용어 정리
인공지능 개발에 앞서 선수지식에 대한 내용을 정리했습니다.

### 모델
모델은 데이터로부터 학습하고 패턴을 추출하여 특정 작업을 수행하는 데 사용되는 알고리즘 또는 구조를 의미합니다. 간단히 말해서, 모델은 `입력` 데이터를 받아들이고 이를 처리하여 원하는 `출력`을 생성하는 방법을 나타내는 것입니다.

### 인공신경망
인공 신경망은 인간의 뇌의 신경 구조에서 영감을 받은 컴퓨터 모델로, 패턴 인식, 데이터 처리 및 학습과 같은 작업을 수행하는 머신 러닝의 한 유형입니다. 인공 신경망은 여러 개의 연결된 노드(뉴런)로 구성되어 있으며, 이러한 뉴런 간의 연결은 가중치로 표현됩니다.

### 인공신경망
딥러닝은 머신 러닝의 한 분야로서, 인공 신경망의 구조를 활용하여 다양한 복잡한 문제를 해결하는 기술입니다. 딥러닝은 이름 그대로 "깊은" 구조의 신경망을 사용하여 데이터의 복잡한 특징을 자동으로 학습하고 패턴을 인식하는 능력을 갖춥니다. 이러한 특성은 이미지, 음성, 텍스트 등 다양한 유형의 데이터에서 높은 수준의 성능을 보이며, 컴퓨터 비전, 음성 인식, 자연어 처리와 같은 분야에서 혁신적인 결과를 이끌어내는 중요한 기술로 자리 잡았습니다.

### RNN
RNN은 `"Recurrent Neural Network"`의 약자로, 순환 신경망을 나타냅니다. RNN은 시퀀스 데이터나 순차적 데이터를 처리하고 `이전 단계의 정보를 현재 단계로 전달`하여 패턴을 학습하는 데 사용되는 신경망 구조입니다.

### LSTM
LSTM은 `"Long Short-Term Memory"`의 약자로, 순환 신경망 (Recurrent Neural Network, RNN)의 한 종류입니다. LSTM은 `RNN의 한계 중 하나인 장기 의존성 문제를 해결`하고, 시퀀스 데이터나 순차적 데이터에서 장기적인 패턴을 학습하기 위해 고안된 구조입니다.

### Transformer
트랜스포머(Transformer)는 인공지능 (AI) 분야에서 자연어 처리와 기계 번역과 같은 작업에 혁명적인 영향을 미친 모델 구조입니다. 2017년에 "Attention is All You Need" 논문에서 제안되었으며, 주로 자연어 처리에서 사용되었지만 이미지 처리와 음성 처리에도 확장되어 활용되고 있습니다.  
트랜스포머는 "인코더-디코더" 구조로 사용되며, 번역이나 문장 생성과 같은 작업에 적합합니다.

### BERT
BERT`(Bidirectional Encoder Representations from Transformers)`는 2018년에 Google에서 발표한 사전 훈련된 언어 모델입니다. BERT는 자연어 처리 분야에서 엄청난 성능 향상을 이끈 모델로, 다양한 언어 이해와 생성 작업에 사용됩니다. 이 모델은 Transformer 아키텍처의 `인코더`를 기반으로 하며, 기존의 언어 모델과는 다르게 양방향 문맥 이해 능력을 가지고 있습니다.

### GPT
GPT는 `"Generative Pre-trained Transformer"`의 약어입니다. 이는 인공지능 언어 모델의 한 형태로, 자연어 처리와 생성 작업을 수행하는 데 사용됩니다. "Generative"는 모델이 새로운 텍스트를 생성할 수 있다는 것을 나타내며, "Pre-trained"는 모델이 대규모 텍스트 데이터를 학습한 후에 새로운 작업에 활용될 수 있다는 것을 의미합니다. "Transformer"는 이 모델의 아키텍처 유형을 나타내며, 주로 시퀀스 데이터 처리에 적합한 구조입니다.

### Hugging Face
허깅페이스(Hugging Face)는 자연어 처리와 인공지능 관련 오픈 소스 소프트웨어를 개발하고 공유하는 기업 및 커뮤니티입니다. 허깅페이스는 다양한 NLP 모델과 도구를 개발하고 배포함으로써 연구원, 개발자, 데이터 과학자 등이 자연어 처리 작업을 보다 쉽게 수행하고 모델을 활용할 수 있도록 돕고 있습니다.